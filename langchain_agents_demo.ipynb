{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomPTech/Langchain-Agents-Demo/blob/main/langchain_agents_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "id": "uZR3iGJJtdDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9261fe-bdd6-4995-c3f7-710ffca3e74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain==0.3.25 \\\n",
        "  langchain-openai==0.3.22 \\\n",
        "  langchain-experimental==0.3.4 \\\n",
        "  numexpr==2.11.0 \\\n",
        "  google-search-results==2.4.2 \\\n",
        "  wikipedia==1.4.0 \\\n",
        "  sqlalchemy==2.0.41"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, you need to use an OpenAI LLM. Here we setup the LLM used for the whole project, with the api key being stored as a Colab Secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02c4fa2",
      "metadata": {
        "id": "c02c4fa2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "    or userdata.get('APIKey')\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bfcbb6",
      "metadata": {
        "id": "73bfcbb6"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the model\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name=\"gpt-4.1-mini\",\n",
        "    temperature=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b1f17c",
      "metadata": {
        "id": "61b1f17c"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import MetaData\n",
        "\n",
        "metadata_obj = MetaData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc1d80e",
      "metadata": {
        "id": "3cc1d80e"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import Column, Integer, String, Table, Date, Float\n",
        "\n",
        "# Define the table\n",
        "temperatures = Table(\n",
        "    \"temperatures\",\n",
        "    metadata_obj,\n",
        "    Column(\"obs_id\", Integer, primary_key=True),\n",
        "    Column(\"city\", String(20), nullable=False),\n",
        "    Column(\"temperature_c\", Float, nullable=False),\n",
        "    Column(\"date\", Date, nullable=False),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a9571a",
      "metadata": {
        "id": "c9a9571a"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\")\n",
        "metadata_obj.create_all(engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c3081f",
      "metadata": {
        "id": "81c3081f"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Example observations\n",
        "observations = [\n",
        "    [1, 'Springfield', 5.0, datetime(2023, 1, 1)],\n",
        "    [2, 'Springfield', 6.5, datetime(2023, 1, 2)],\n",
        "    [3, 'Springfield', 4.0, datetime(2023, 1, 3)],\n",
        "    [4, 'Springfield', 3.5, datetime(2023, 1, 4)],\n",
        "    [5, 'Springfield', 2.0, datetime(2023, 1, 5)],\n",
        "    [6, 'Shelbyville', -1.0, datetime(2023, 1, 1)],\n",
        "    [7, 'Shelbyville', -0.5, datetime(2023, 1, 2)],\n",
        "    [8, 'Shelbyville', -2.0, datetime(2023, 1, 3)],\n",
        "    [9, 'Shelbyville', -3.0, datetime(2023, 1, 4)],\n",
        "    [10, 'Shelbyville', -4.0, datetime(2023, 1, 5)],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fd20fa",
      "metadata": {
        "id": "85fd20fa"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import insert\n",
        "\n",
        "# Function to insert observations\n",
        "def insert_obs(obs):\n",
        "    stmt = insert(temperatures).values(\n",
        "        obs_id=obs[0],\n",
        "        city=obs[1],\n",
        "        temperature_c=obs[2],\n",
        "        date=obs[3]\n",
        "    )\n",
        "\n",
        "    with engine.begin() as conn:\n",
        "        conn.execute(stmt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6766f1f7",
      "metadata": {
        "id": "6766f1f7"
      },
      "outputs": [],
      "source": [
        "for obs in observations:\n",
        "    insert_obs(obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9721648e",
      "metadata": {
        "id": "9721648e"
      },
      "outputs": [],
      "source": [
        "from langchain.utilities import SQLDatabase\n",
        "from langchain_experimental.sql import SQLDatabaseChain\n",
        "\n",
        "db = SQLDatabase(engine)\n",
        "sql_chain = SQLDatabaseChain.from_llm(llm=llm, db=db, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eaf306a",
      "metadata": {
        "id": "1eaf306a"
      },
      "source": [
        "### Agent type #1: Zero Shot React"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mie_37ERl_ac",
      "metadata": {
        "id": "mie_37ERl_ac"
      },
      "source": [
        "The **Zero-Shot Agent** gets its name because it can attempt a task immediately without needing prior examples or multiple interactions.\n",
        "\n",
        "- **Zero-shot** → The agent sees the input **once** and produces an answer right away. It doesn’t learn from previous steps or examples.\n",
        "- **No memory** → Unlike a conversational agent, it **cannot recall previous questions or answers**. Each interaction is independent.\n",
        "\n",
        "Basically, it's like asking someone a question without giving any context or hints and expecting them to answer correctly the first time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tgn6dRLEcxli",
      "metadata": {
        "id": "Tgn6dRLEcxli"
      },
      "source": [
        "This method uses a *toolkit* (a set of related tools designed to be used together) instead of a custom array of `tools`. For this use case, we will use `SQLDatabaseToolkit`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GtSMUCaIlupp",
      "metadata": {
        "id": "GtSMUCaIlupp"
      },
      "source": [
        "**Important Note:** *When interacting with agents, it is incredibly important to set the `max_iterations` parameters because agents can get stuck in infinite loops, which will consume your API tokens (and either exceed your quota or cost you more moeny). The default value is 15 to allow for many tools and complex reasoning, but for most applications you should keep it much lower.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Z4EmMmqiOvZ",
      "metadata": {
        "id": "5Z4EmMmqiOvZ"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_sql_agent\n",
        "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "agent_executor = create_sql_agent(\n",
        "    llm=llm,\n",
        "    toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    max_iterations=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iBqz6aFzj-2d",
      "metadata": {
        "id": "iBqz6aFzj-2d"
      },
      "source": [
        "Let's test our newly created agent by asking it a question that involves a math operation over the temperatures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MdvgpwHRic3W",
      "metadata": {
        "id": "MdvgpwHRic3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ba1396-f8ae-4d4e-eacc-a5c5c3a57091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction: sql_db_list_tables\n",
            "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mtemperatures\u001b[0m\u001b[32;1m\u001b[1;3mThought: There is only one table named \"temperatures.\" I should check the schema of this table to understand what columns it contains and how to query temperature data for Springfield and Shelbyville on January 3rd and 4th.\n",
            "Action: sql_db_schema\n",
            "Action Input: temperatures\u001b[0m\u001b[33;1m\u001b[1;3m\n",
            "CREATE TABLE temperatures (\n",
            "\tobs_id INTEGER NOT NULL, \n",
            "\tcity VARCHAR(20) NOT NULL, \n",
            "\ttemperature_c FLOAT NOT NULL, \n",
            "\tdate DATE NOT NULL, \n",
            "\tPRIMARY KEY (obs_id)\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from temperatures table:\n",
            "obs_id\tcity\ttemperature_c\tdate\n",
            "1\tSpringfield\t5.0\t2023-01-01\n",
            "2\tSpringfield\t6.5\t2023-01-02\n",
            "3\tSpringfield\t4.0\t2023-01-03\n",
            "*/\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to get the temperatures for Springfield and Shelbyville on January 3rd and January 4th, then calculate the difference in temperature between the two cities on each day, and finally multiply those two differences.\n",
            "\n",
            "Action: sql_db_query_checker\n",
            "Action Input: \n",
            "SELECT city, temperature_c, date FROM temperatures WHERE city IN ('Springfield', 'Shelbyville') AND date IN ('2023-01-03', '2023-01-04') ORDER BY date, city LIMIT 10;\u001b[0m\u001b[36;1m\u001b[1;3mSELECT city, temperature_c, date FROM temperatures WHERE city IN ('Springfield', 'Shelbyville') AND date IN ('2023-01-03', '2023-01-04') ORDER BY date, city LIMIT 10;\u001b[0m\u001b[32;1m\u001b[1;3mAction: sql_db_query\n",
            "Action Input: \n",
            "SELECT city, temperature_c, date FROM temperatures WHERE city IN ('Springfield', 'Shelbyville') AND date IN ('2023-01-03', '2023-01-04') ORDER BY date, city LIMIT 10;\u001b[0m\u001b[36;1m\u001b[1;3m[('Shelbyville', -2.0, '2023-01-03'), ('Springfield', 4.0, '2023-01-03'), ('Shelbyville', -3.0, '2023-01-04'), ('Springfield', 3.5, '2023-01-04')]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The temperature difference on January 3rd is 4.0 - (-2.0) = 6.0 degrees. The temperature difference on January 4th is 3.5 - (-3.0) = 6.5 degrees. The product of these differences is 6.0 * 6.5 = 39.0.\n",
            "\n",
            "Final Answer: The difference in temperature between Springfield and Shelbyville on January 3rd, multiplied by the difference on January 4th, is 39.0 degrees.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Result: The difference in temperature between Springfield and Shelbyville on January 3rd, multiplied by the difference on January 4th, is 39.0 degrees.\n"
          ]
        }
      ],
      "source": [
        "result = agent_executor.invoke(\n",
        "    \"What is the difference in temperature between Springfield and Shelbyville on January 3rd, \"\n",
        "    \"multiplied by the difference on January 4th?\"\n",
        ")\n",
        "print(f\"Result: {result['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d70642",
      "metadata": {
        "id": "04d70642"
      },
      "source": [
        "### Agent type #2: Conversational React"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec589b1",
      "metadata": {
        "id": "1ec589b1"
      },
      "source": [
        "If we want an AI assistant that can remember what we’ve talked about and reason about tasks, we can use a **Conversational ReAct Agent** in LangChain.\n",
        "\n",
        "- **Conversational** → The agent keeps track of the **chat history**, allowing it to respond in context and remember details from earlier in the conversation.\n",
        "\n",
        "- **ReAct** (short for **Reason + Act**) → The agent doesn’t just answer directly. Instead, it:\n",
        "  1. **Reasons** about the problem step by step.\n",
        "  2. **Decides** if it needs to use a tool (like a database, calculator, or search engine).\n",
        "  3. **Acts** by calling the tool.\n",
        "  4. **Continues reasoning** with the result to produce the final answer.\n",
        "\n",
        "This combination makes the agent feel like a smart assistant that can chat naturally and performs tasks autonomously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c26c50f",
      "metadata": {
        "id": "5c26c50f"
      },
      "source": [
        "We will use the math tool in this example and load it as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b6faff3",
      "metadata": {
        "id": "4b6faff3"
      },
      "outputs": [],
      "source": [
        "# Use the modern tool-based approach instead of deprecated LLMMathChain\n",
        "from langchain_core.tools import tool\n",
        "import math\n",
        "import numexpr\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Calculate expression using Python's numexpr library.\n",
        "\n",
        "    Expression should be a single line mathematical expression\n",
        "    that solves the problem.\n",
        "\n",
        "    Examples:\n",
        "        \"37593 * 67\" for \"37593 times 67\"\n",
        "        \"37593**(1/5)\" for \"37593^(1/5)\"\n",
        "        \"10000 * (1 + 0.08)**5\" for compound interest\n",
        "    \"\"\"\n",
        "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
        "    return str(\n",
        "        numexpr.evaluate(\n",
        "            expression.strip(),\n",
        "            global_dict={},  # restrict access to globals\n",
        "            local_dict=local_dict,  # add common mathematical functions\n",
        "        )\n",
        "    )\n",
        "\n",
        "tools = [calculator]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aff4edf",
      "metadata": {
        "id": "0aff4edf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea87df52-0ea4-4ae2-a37a-6b3d65fe2bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2595623399.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# The memory type being used here is a simple buffer memory to allow us to remember previous steps in the reasoning chain.\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following examples rely on a **scratchpad**, which is like a working memory for the agent while it is reasoning through a problem. It’s a place where the agent can keep track of its intermediate thoughts, calculations, and tool usage before giving a final answer."
      ],
      "metadata": {
        "id": "bh-zMx75PIEU"
      },
      "id": "bh-zMx75PIEU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6579cef0",
      "metadata": {
        "id": "6579cef0"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.base import RunnableSerializable\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Add a final_answer tool\n",
        "@tool\n",
        "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
        "    \"\"\"Use this tool to provide a final answer to the user.\n",
        "    The answer should be in natural language as this will be provided\n",
        "    to the user directly. The tools_used must include a list of tool\n",
        "    names that were used within the `scratchpad`.\n",
        "    \"\"\"\n",
        "    return {\"answer\": answer, \"tools_used\": tools_used}\n",
        "\n",
        "# Add tools\n",
        "tools = [final_answer, calculator]\n",
        "\n",
        "# First, create a prompt that forces the LLM to analyze the history\n",
        "history_analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"Analyze the conversation history below and identify any calculations that have already been performed. \"\n",
        "        \"Extract the results of these calculations so they can be reused instead of recalculating.\"\n",
        "    )),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"What calculations have been done and what are their results?\"),\n",
        "])\n",
        "\n",
        "# Then the main agent prompt\n",
        "agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You're a helpful assistant. When answering a user's question \"\n",
        "        \"you should first use one of the tools provided. After using a \"\n",
        "        \"tool the tool output will be provided in the \"\n",
        "        \"'scratchpad' below. If you have an answer in the \"\n",
        "        \"scratchpad you should not use any more tools and \"\n",
        "        \"instead answer directly to the user. \"\n",
        "        \"IMPORTANT: Use the analysis of previous calculations to avoid recalculating.\"\n",
        "    )),\n",
        "    (\"human\", \"Previous calculations analysis: {history_analysis}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "# define the agent runnable with history analysis first\n",
        "agent: RunnableSerializable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | {\n",
        "        \"history_analysis\": lambda x: llm.invoke(history_analysis_prompt.format_messages(\n",
        "            chat_history=x[\"chat_history\"]\n",
        "        )).content,\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | agent_prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"auto\")\n",
        ")\n",
        "\n",
        "# create tool name to function mapping as per guide\n",
        "name2tool = {tool.name: tool.func for tool in tools}\n",
        "\n",
        "class CustomAgentExecutor:\n",
        "    def __init__(self, max_iterations: int = 5):\n",
        "        self.max_iterations = max_iterations\n",
        "        self.chat_history = []  # Simple list to store conversation history\n",
        "        self.agent = agent\n",
        "\n",
        "    def invoke(self, input: str) -> dict:\n",
        "        # invoke the agent but we do this iteratively in a loop until\n",
        "        # reaching a final answer\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "\n",
        "        while count < self.max_iterations:\n",
        "            # invoke a step for the agent to generate a tool call\n",
        "            tool_call = self.agent.invoke({\n",
        "                \"input\": input,\n",
        "                \"chat_history\": self.chat_history,\n",
        "                \"agent_scratchpad\": agent_scratchpad\n",
        "            })\n",
        "\n",
        "            # add initial tool call to scratchpad\n",
        "            agent_scratchpad.append(tool_call)\n",
        "\n",
        "            # Handle ALL tool calls, not just the first one\n",
        "            if tool_call.tool_calls:\n",
        "                for tool_call_obj in tool_call.tool_calls:\n",
        "                    tool_name = tool_call_obj[\"name\"]\n",
        "                    tool_args = tool_call_obj[\"args\"]\n",
        "                    tool_call_id = tool_call_obj[\"id\"]\n",
        "\n",
        "                    # execute the tool\n",
        "                    tool_out = name2tool[tool_name](**tool_args)\n",
        "\n",
        "                    # add the tool output to the agent scratchpad\n",
        "                    tool_exec = ToolMessage(\n",
        "                        content=f\"{tool_out}\",\n",
        "                        tool_call_id=tool_call_id\n",
        "                    )\n",
        "                    agent_scratchpad.append(tool_exec)\n",
        "\n",
        "                    # add a print so we can see intermediate steps\n",
        "                    print(f\"{count}: {tool_name}({tool_args}) -> {tool_out}\")\n",
        "\n",
        "                count += 1\n",
        "\n",
        "                # Check if any tool call is the final answer tool\n",
        "                if any(tc[\"name\"] == \"final_answer\" for tc in tool_call.tool_calls):\n",
        "                    # Get the final answer from the final_answer tool\n",
        "                    final_tool_call = next(tc for tc in tool_call.tool_calls if tc[\"name\"] == \"final_answer\")\n",
        "                    final_answer = final_tool_call[\"args\"][\"answer\"]\n",
        "                    break\n",
        "            else:\n",
        "                # no tool call, we have a final answer\n",
        "                final_answer = tool_call.content\n",
        "                break\n",
        "\n",
        "        # Add to conversation history ONLY the human input and final AI response\n",
        "        # This preserves memory without corrupting it with tool calls\n",
        "        self.chat_history.extend([\n",
        "            HumanMessage(content=input),\n",
        "            AIMessage(content=final_answer)\n",
        "        ])\n",
        "\n",
        "        # return the final answer in dict form\n",
        "        return {\"output\": final_answer}\n",
        "\n",
        "# Initialize the custom agent executor\n",
        "conversational_agent = CustomAgentExecutor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cabbea50",
      "metadata": {
        "id": "cabbea50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d312fe9b-20ca-42c6-c49d-1d4b2b15bb6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: calculator({'expression': '10000 * (1 + 0.08)**5'}) -> 14693.280768000006\n",
            "1: final_answer({'answer': 'The value of 10000 * (1 + 0.08)^5 is approximately 14693.28.', 'tools_used': ['functions.calculator']}) -> {'answer': 'The value of 10000 * (1 + 0.08)^5 is approximately 14693.28.', 'tools_used': ['functions.calculator']}\n",
            "Result: The value of 10000 * (1 + 0.08)^5 is approximately 14693.28.\n"
          ]
        }
      ],
      "source": [
        "# First question\n",
        "result = conversational_agent.invoke(\"What is 10000 * (1 + 0.08)**5?\")\n",
        "print(f\"Result: {result['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ee8041",
      "metadata": {
        "id": "32ee8041"
      },
      "source": [
        "Let's see what happens if we try to answer the question that is related to the previous one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e109878",
      "metadata": {
        "id": "5e109878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078f5466-c212-4173-8dcb-8bd1d8e8cade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: calculator({'expression': '15000 * (1 + 0.08)**5 - 14693.28'}) -> 7346.641152000009\n",
            "1: final_answer({'answer': 'If we start with $15,000 and follow the same 8% annual growth for 5 years with compound interest, we would have approximately $7,346.64 more compared to the previous scenario where we started with $10,000.', 'tools_used': ['functions.calculator']}) -> {'answer': 'If we start with $15,000 and follow the same 8% annual growth for 5 years with compound interest, we would have approximately $7,346.64 more compared to the previous scenario where we started with $10,000.', 'tools_used': ['functions.calculator']}\n",
            "Result: If we start with $15,000 and follow the same 8% annual growth for 5 years with compound interest, we would have approximately $7,346.64 more compared to the previous scenario where we started with $10,000.\n"
          ]
        }
      ],
      "source": [
        "result = conversational_agent.invoke(\n",
        "    \"If we start with $15,000 instead and follow the same 8% annual growth for 5 years with compound interest, how much more would we have compared to the previous scenario?\"\n",
        ")\n",
        "print(f\"Result: {result['output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44135b8d",
      "metadata": {
        "id": "44135b8d"
      },
      "source": [
        "### Agent type #3: React Docstore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e6d0d13",
      "metadata": {
        "id": "1e6d0d13"
      },
      "source": [
        "A **ReAct Docstore Agent** is an AI assistant that can search through documents and reason about the information it finds to answer questions. It only has two functions: \"Search\" and \"Lookup.\"\n",
        "\n",
        "A **Docstore** is a database or collection of documents that the agent can query to find information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d569fd41",
      "metadata": {
        "id": "d569fd41"
      },
      "source": [
        "With \"Search\" it will bring up a relevant article and with \"Lookup\" the agent will find the right piece of information in the article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc452af",
      "metadata": {
        "id": "ecc452af"
      },
      "outputs": [],
      "source": [
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def Search(query: str) -> str:\n",
        "    \"\"\"Search Wikipedia for information about a topic.\"\"\"\n",
        "    try:\n",
        "        wiki = WikipediaAPIWrapper()\n",
        "        return wiki.run(query)\n",
        "    except Exception as e:\n",
        "        return f\"Error searching Wikipedia: {e}\"\n",
        "\n",
        "@tool\n",
        "def Lookup(term: str) -> str:\n",
        "    \"\"\"Look up a specific term or phrase in Wikipedia.\"\"\"\n",
        "    try:\n",
        "        wiki = WikipediaAPIWrapper()\n",
        "        return wiki.run(term)\n",
        "    except Exception as e:\n",
        "        return f\"Error looking up term: {e}\"\n",
        "\n",
        "tools = [Search, Lookup]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595938a1",
      "metadata": {
        "id": "595938a1"
      },
      "outputs": [],
      "source": [
        "# Create a custom agent executor for docstore tools\n",
        "docstore_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You're a helpful assistant that can search and lookup information.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "docstore_agent_runnable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | docstore_prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"auto\")\n",
        ")\n",
        "\n",
        "docstore_name2tool = {tool.name: tool.func for tool in tools}\n",
        "\n",
        "class DocstoreAgentExecutor:\n",
        "    def __init__(self, max_iterations: int = 3):\n",
        "        self.max_iterations = max_iterations\n",
        "        self.agent = docstore_agent_runnable\n",
        "\n",
        "    def invoke(self, input: str) -> dict:\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "\n",
        "        while count < self.max_iterations:\n",
        "            tool_call = self.agent.invoke({\n",
        "                \"input\": input,\n",
        "                \"agent_scratchpad\": agent_scratchpad\n",
        "            })\n",
        "\n",
        "            agent_scratchpad.append(tool_call)\n",
        "\n",
        "            if not tool_call.tool_calls:\n",
        "                final_answer = tool_call.content\n",
        "                break\n",
        "\n",
        "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
        "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
        "            tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
        "            tool_out = docstore_name2tool[tool_name](**tool_args)\n",
        "\n",
        "            tool_exec = ToolMessage(\n",
        "                content=f\"{tool_out}\",\n",
        "                tool_call_id=tool_call_id\n",
        "            )\n",
        "            agent_scratchpad.append(tool_exec)\n",
        "\n",
        "            print(f\"{count}: {tool_name}({tool_args}) = {tool_out[:100]}...\")\n",
        "            count += 1\n",
        "\n",
        "        return {\"input\": input, \"output\": final_answer}\n",
        "\n",
        "docstore_agent = DocstoreAgentExecutor()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can ask it a question about when the last Apollo mission was."
      ],
      "metadata": {
        "id": "GMyypMezPXB5"
      },
      "id": "GMyypMezPXB5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba6b065",
      "metadata": {
        "id": "bba6b065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970ded41-a3a9-434d-afea-25e4815a8c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Lookup({'term': 'Apollo program'}) = Page: Apollo program\n",
            "Summary: The Apollo program, also known as Project Apollo, was the United State...\n",
            "Result: The last Apollo mission was Apollo 17, which took place in December 1972. It was the final mission of the Apollo program that landed astronauts on the Moon.\n"
          ]
        }
      ],
      "source": [
        "result = docstore_agent.invoke(\"When was the last Apollo mission?\")\n",
        "print(f\"Result: {result['output']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pinecone1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}